{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb632fc3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-19T22:38:56.819526Z",
     "iopub.status.busy": "2025-07-19T22:38:56.819240Z",
     "iopub.status.idle": "2025-07-19T22:45:14.502736Z",
     "shell.execute_reply": "2025-07-19T22:45:14.501955Z"
    },
    "papermill": {
     "duration": 377.688438,
     "end_time": "2025-07-19T22:45:14.504043",
     "exception": false,
     "start_time": "2025-07-19T22:38:56.815605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.3.2\r\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting imbalanced-learn==0.12.3\r\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (3.6.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2024.2.0)\r\n",
      "Downloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "  Attempting uninstall: imbalanced-learn\r\n",
      "    Found existing installation: imbalanced-learn 0.13.0\r\n",
      "    Uninstalling imbalanced-learn-0.13.0:\r\n",
      "      Successfully uninstalled imbalanced-learn-0.13.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed imbalanced-learn-0.12.3 scikit-learn-1.3.2\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-19 22:39:08.176419: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752964748.364226      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752964748.417591      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully imported SMOTE.\n",
      "Inspecting first 2 rows of first GeoJSON file:\n",
      "\n",
      "Row 0:\n",
      "  Band B1: shape=(5, 5), first few values=[0.1118 0.1158 0.1158 0.1158 0.1158]\n",
      "  Band B2: shape=(5, 5), first few values=[0.11225 0.11335 0.11335 0.11955 0.11955]\n",
      "  Band B11: shape=(5, 5), first few values=[0.17015 0.1891  0.1891  0.1891  0.1891 ]\n",
      "  Band NDVI: shape=(5, 5), first few values=[0.27416557 0.2273243  0.2273243  0.28395182 0.28395182]\n",
      "  Band DEM: shape=(5, 5), first few values=[88. 88. 88. 88. 88.]\n",
      "  Band B2_1: shape=(), first few values=[nan]\n",
      "  Band NDVI_7: shape=(), first few values=[nan]\n",
      "\n",
      "Row 1:\n",
      "  Band B1: shape=(5, 5), first few values=[0.125 0.124 0.124 0.124 0.124]\n",
      "  Band B2: shape=(5, 5), first few values=[0.1239 0.1176 0.1176 0.1152 0.117 ]\n",
      "  Band B11: shape=(5, 5), first few values=[0.1976 0.1797 0.1797 0.1797 0.1779]\n",
      "  Band NDVI: shape=(5, 5), first few values=[0.40535372 0.36756483 0.36756483 0.36180538 0.34991294]\n",
      "  Band DEM: shape=(5, 5), first few values=[94. 94. 93. 93. 93.]\n",
      "  Band B2_1: shape=(5, 5), first few values=[0.12365 0.11975 0.11975 0.1182  0.1181 ]\n",
      "  Band NDVI_7: shape=(5, 5), first few values=[0.3888209  0.35329902 0.35329902 0.3116279  0.3292073 ]\n",
      "Processing file: needleleaf_douglas fir_douglas firmar-oct-2022.geojson, Rows: 2185\n",
      "Processing file: broadleaf_short-lived deciduous_aldermar-oct-2022.geojson, Rows: 2143\n",
      "Processing file: broadleaf_beech_european beechmar-oct-2022.geojson, Rows: 4756\n",
      "Processing file: needleleaf_larch_japanese larchmar-oct-2022.geojson, Rows: 1613\n",
      "Processing file: broadleaf_short-lived deciduous_poplarmar-oct-2022.geojson, Rows: 387\n",
      "Processing file: needleleaf_pine_scots pinemar-oct-2022.geojson, Rows: 5389\n",
      "Processing file: broadleaf_oak_english oakmar-oct-2022.geojson, Rows: 2808\n",
      "Processing file: needleleaf_larch_european larchmar-oct-2022.geojson, Rows: 1139\n",
      "Processing file: broadleaf_long-lived deciduous_sycamore maplemar-oct-2022.geojson, Rows: 2096\n",
      "Processing file: broadleaf_long-lived deciduous_european ashmar-oct-2022.geojson, Rows: 2202\n",
      "Processing file: broadleaf_short-lived deciduous_birchmar-oct-2022.geojson, Rows: 2468\n",
      "Processing file: broadleaf_long-lived deciduous_lindenmar-oct-2022.geojson, Rows: 161\n",
      "Processing file: broadleaf_oak_red oakmar-oct-2022.geojson, Rows: 1460\n",
      "Processing file: needleleaf_spruce_norway sprucemar-oct-2022.geojson, Rows: 5037\n",
      "Processing file: broadleaf_oak_sessile oakmar-oct-2022.geojson, Rows: 2115\n",
      "Processing file: needleleaf_pine_weymouth pinemar-oct-2022.geojson, Rows: 478\n",
      "Processing file: needleleaf_fir_silver firmar-oct-2022.geojson, Rows: 811\n",
      "Processing file: broadleaf_long-lived deciduous_cherrymar-oct-2022.geojson, Rows: 247\n",
      "Processing file: needleleaf_pine_black pinemar-oct-2022.geojson, Rows: 412\n",
      "\n",
      "Total samples attempted: 37907\n",
      "Valid samples processed: 37907\n",
      "\n",
      "Bands with None or missing values:\n",
      "  B2_4: 25577 times\n",
      "  B3_4: 25577 times\n",
      "  B4_4: 25577 times\n",
      "  B5_4: 25577 times\n",
      "  B6_4: 25577 times\n",
      "  B7_4: 25577 times\n",
      "  B8_4: 25577 times\n",
      "  B8A_4: 25577 times\n",
      "  B11_4: 25577 times\n",
      "  B12_4: 25577 times\n",
      "  NDVI_4: 25577 times\n",
      "  EVI_4: 25577 times\n",
      "  SAVI_4: 25577 times\n",
      "  NDWI_4: 25577 times\n",
      "  B1_4: 25569 times\n",
      "  B9_4: 25569 times\n",
      "  DEM_4: 23721 times\n",
      "  B2_7: 21786 times\n",
      "  B3_7: 21786 times\n",
      "  B4_7: 21786 times\n",
      "  B5_7: 21786 times\n",
      "  B6_7: 21786 times\n",
      "  B7_7: 21786 times\n",
      "  B8_7: 21786 times\n",
      "  B8A_7: 21786 times\n",
      "  B11_7: 21786 times\n",
      "  B12_7: 21786 times\n",
      "  NDVI_7: 21786 times\n",
      "  EVI_7: 21786 times\n",
      "  SAVI_7: 21786 times\n",
      "  NDWI_7: 21786 times\n",
      "  B1_7: 21781 times\n",
      "  B9_7: 21781 times\n",
      "  DEM_7: 20915 times\n",
      "  B2_1: 5484 times\n",
      "  B3_1: 5484 times\n",
      "  B4_1: 5484 times\n",
      "  B5_1: 5484 times\n",
      "  B6_1: 5484 times\n",
      "  B7_1: 5484 times\n",
      "  B8_1: 5484 times\n",
      "  B8A_1: 5484 times\n",
      "  B11_1: 5484 times\n",
      "  B12_1: 5484 times\n",
      "  NDVI_1: 5484 times\n",
      "  EVI_1: 5484 times\n",
      "  SAVI_1: 5484 times\n",
      "  NDWI_1: 5484 times\n",
      "  B1_1: 5465 times\n",
      "  B9_1: 5464 times\n",
      "  B2_6: 5256 times\n",
      "  B3_6: 5256 times\n",
      "  B4_6: 5256 times\n",
      "  B5_6: 5256 times\n",
      "  B6_6: 5256 times\n",
      "  B7_6: 5256 times\n",
      "  B8_6: 5256 times\n",
      "  B8A_6: 5256 times\n",
      "  B11_6: 5256 times\n",
      "  B12_6: 5256 times\n",
      "  NDVI_6: 5256 times\n",
      "  EVI_6: 5256 times\n",
      "  SAVI_6: 5256 times\n",
      "  NDWI_6: 5256 times\n",
      "  B1_6: 5255 times\n",
      "  B9_6: 5255 times\n",
      "  DEM_1: 3956 times\n",
      "  DEM_6: 1460 times\n",
      "  B1_2: 389 times\n",
      "  B2_2: 389 times\n",
      "  B3_2: 389 times\n",
      "  B4_2: 389 times\n",
      "  B5_2: 389 times\n",
      "  B6_2: 389 times\n",
      "  B7_2: 389 times\n",
      "  B8_2: 389 times\n",
      "  B8A_2: 389 times\n",
      "  B9_2: 389 times\n",
      "  B11_2: 389 times\n",
      "  B12_2: 389 times\n",
      "  NDVI_2: 389 times\n",
      "  EVI_2: 389 times\n",
      "  SAVI_2: 389 times\n",
      "  NDWI_2: 389 times\n",
      "  B2_3: 6 times\n",
      "  B3_3: 6 times\n",
      "  B4_3: 6 times\n",
      "  B5_3: 6 times\n",
      "  B6_3: 6 times\n",
      "  B7_3: 6 times\n",
      "  B8_3: 6 times\n",
      "  B8A_3: 6 times\n",
      "  B11_3: 6 times\n",
      "  B12_3: 6 times\n",
      "  NDVI_3: 6 times\n",
      "  EVI_3: 6 times\n",
      "  SAVI_3: 6 times\n",
      "  NDWI_3: 6 times\n",
      "  B1_3: 5 times\n",
      "  B9_3: 5 times\n",
      "\n",
      "Valid samples per species:\n",
      "  scots pine: 5389\n",
      "  norway spruce: 5037\n",
      "  european beech: 4756\n",
      "  english oak: 2808\n",
      "  birch: 2468\n",
      "  european ash: 2202\n",
      "  douglas fir: 2185\n",
      "  alder: 2143\n",
      "  sessile oak: 2115\n",
      "  sycamore maple: 2096\n",
      "  japanese larch: 1613\n",
      "  red oak: 1460\n",
      "  european larch: 1139\n",
      "  silver fir: 811\n",
      "  weymouth pine: 478\n",
      "  black pine: 412\n",
      "  poplar: 387\n",
      "  cherry: 247\n",
      "  linden: 161\n"
     ]
    }
   ],
   "source": [
    "# ********************************************************************************\n",
    "# IMPORTANT: BEFORE RUNNING THIS CELL, PERFORM A \"FACTORY RESET RUNTIME\" (Colab)\n",
    "# OR THE EQUIVALENT DEEPEST RESTART IN YOUR ENVIRONMENT (e.g., Kaggle Session Restart).\n",
    "# THEN, RUN THIS CELL AS THE VERY FIRST CODE IN YOUR NOTEBOOK.\n",
    "# ********************************************************************************\n",
    "!pip install -U scikit-learn==1.3.2 imbalanced-learn==0.12.3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from collections import Counter\n",
    "import ast\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "# Now, the imports should work if the environment is truly clean\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    print(\"\\nSuccessfully imported SMOTE.\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\nCRITICAL ERROR: Failed to import SMOTE even after aggressive reinstallation: {e}\")\n",
    "    print(\"This indicates a severe, persistent environment issue.\")\n",
    "    print(\"Please double-check that you performed a 'Factory reset runtime' (Colab) or equivalent.\")\n",
    "    exit()\n",
    "\n",
    "# 1. Inspect and Load GeoJSON Files (Modified for Zero Imputation)\n",
    "data_dir = \"/kaggle/input/mar-oct\"  # Replace with your folder path\n",
    "all_features = []\n",
    "all_labels = []\n",
    "invalid_samples = []\n",
    "invalid_bands = Counter()\n",
    "species_counts = Counter()\n",
    "\n",
    "# Updated bands list to include all relevant bands\n",
    "bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'SAVI', 'NDWI', 'DEM']\n",
    "months = ['', '_1', '_2', '_3', '_4', '_5', '_6', '_7']\n",
    "band_columns = [band + month for month in months for band in bands]\n",
    "\n",
    "# Inspect first file\n",
    "first_file = os.path.join(data_dir, os.listdir(data_dir)[0]) if os.listdir(data_dir) else None\n",
    "if first_file and first_file.endswith(\".geojson\"):\n",
    "    gdf = gpd.read_file(first_file)\n",
    "    print(\"Inspecting first 2 rows of first GeoJSON file:\")\n",
    "    for idx in range(min(2, len(gdf))):\n",
    "        print(f\"\\nRow {idx}:\")\n",
    "        for band in ['B1', 'B2', 'B11', 'NDVI', 'DEM', 'B2_1', 'NDVI_7']:\n",
    "            if band in gdf.columns:\n",
    "                data = gdf[band].iloc[idx]\n",
    "                try:\n",
    "                    parsed_data = ast.literal_eval(data) if isinstance(data, str) else data\n",
    "                    array = np.array(parsed_data, dtype=np.float32)\n",
    "                    print(f\"  Band {band}: shape={array.shape}, first few values={array.flatten()[:5]}\")\n",
    "                except (ValueError, SyntaxError, TypeError) as e:\n",
    "                    print(f\"  Band {band}: Error parsing/converting: {e}\")\n",
    "            else:\n",
    "                print(f\"  Band {band}: Not found in GeoJSON file\")\n",
    "\n",
    "# Load all GeoJSON files\n",
    "total_samples_attempted = 0\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".geojson\"):\n",
    "        try:\n",
    "            gdf = gpd.read_file(os.path.join(data_dir, file))\n",
    "            print(f\"Processing file: {file}, Rows: {len(gdf)}\")\n",
    "            total_samples_attempted += len(gdf)\n",
    "            for idx, row in gdf.iterrows():\n",
    "                try:\n",
    "                    patch = []\n",
    "                    for col in band_columns:\n",
    "                        if col not in gdf.columns:\n",
    "                            invalid_bands[col] += 1\n",
    "                            array = np.zeros((5, 5), dtype=np.float32)  # Impute with zeros\n",
    "                            patch.append(array)\n",
    "                            continue\n",
    "                        data = row[col]\n",
    "                        if data is None or (isinstance(data, str) and data.lower() == 'none'):\n",
    "                            invalid_bands[col] += 1\n",
    "                            array = np.zeros((5, 5), dtype=np.float32)  # Impute with zeros\n",
    "                            patch.append(array)\n",
    "                            continue\n",
    "                        try:\n",
    "                            parsed_data = ast.literal_eval(data) if isinstance(data, str) else data\n",
    "                            array = np.array(parsed_data, dtype=np.float32).reshape(5, 5)\n",
    "                        except (ValueError, SyntaxError, TypeError) as e:\n",
    "                            invalid_bands[col] += 1\n",
    "                            array = np.zeros((5, 5), dtype=np.float32)  # Impute for parsing errors\n",
    "                            patch.append(array)\n",
    "                            continue\n",
    "                        patch.append(array)\n",
    "                    patch = np.stack(patch, axis=-1)\n",
    "                    if patch.shape != (5, 5, 136):  # Expected shape: 17 bands * 8 months\n",
    "                        raise ValueError(f\"Unexpected patch shape: {patch.shape}\")\n",
    "                    all_features.append(patch)\n",
    "                    all_labels.append(row['l3_species'])\n",
    "                    species_counts[row['l3_species']] += 1\n",
    "                except (ValueError, SyntaxError, TypeError) as e:\n",
    "                    invalid_samples.append((file, idx, str(e)))\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Log invalid samples and bands\n",
    "print(f\"\\nTotal samples attempted: {total_samples_attempted}\")\n",
    "print(f\"Valid samples processed: {len(all_features)}\")\n",
    "if invalid_samples:\n",
    "    print(f\"\\nSkipped {len(invalid_samples)} invalid samples:\")\n",
    "    for file, idx, error in invalid_samples:\n",
    "        print(f\"File: {file}, Row: {idx}, Error: {error}\")\n",
    "if invalid_bands:\n",
    "    print(\"\\nBands with None or missing values:\")\n",
    "    for band, count in invalid_bands.most_common():\n",
    "        print(f\"  {band}: {count} times\")\n",
    "print(\"\\nValid samples per species:\")\n",
    "for species, count in species_counts.most_common():\n",
    "    print(f\"  {species}: {count}\")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "if not all_features:\n",
    "    print(\"\\nError: No valid samples loaded. Using Random Forest with dummy data.\")\n",
    "    X_dummy = np.random.rand(100, 5*5*136)  # Updated for 136 channels\n",
    "    y_dummy = np.random.randint(0, 5, 100)\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_dummy, y_dummy)\n",
    "    print(\"Random Forest dummy accuracy:\", rf.score(X_dummy, y_dummy))\n",
    "    print(\"Please re-export data with updated GEE code.\")\n",
    "    exit()\n",
    "\n",
    "X = np.array(all_features, dtype=np.float32)  # Shape: (N, 5, 5, 136)\n",
    "y = np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e9bc3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-19T22:45:14.513187Z",
     "iopub.status.busy": "2025-07-19T22:45:14.512946Z",
     "iopub.status.idle": "2025-07-20T02:19:29.102169Z",
     "shell.execute_reply": "2025-07-20T02:19:29.101325Z"
    },
    "papermill": {
     "duration": 12854.595354,
     "end_time": "2025-07-20T02:19:29.103414",
     "exception": false,
     "start_time": "2025-07-19T22:45:14.508060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED TRANSFORMER TRAINING AND EVALUATION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "1. Training Advanced Transformer Model...\n",
      "Starting advanced transformer training pipeline...\n",
      "Applying SMOTE to balance training data...\n",
      "Scaling features...\n",
      "Data shapes - Train: (71668, 3400), Val: (5686, 3400), Test: (5687, 3400)\n",
      "Number of classes: 19\n",
      "Sequence length: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752965126.070526      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 119,188 parameters...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"advanced_transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"advanced_transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_encoding │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEncodin…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ positional_encod… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,472</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,472</span> │ transformer_bloc… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_2 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,472</span> │ transformer_bloc… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ transformer_bloc… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ softmax (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_bloc… │\n",
       "│                     │                   │            │ softmax[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dropout_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,235</span> │ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m128\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ positional_encoding │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mPositionalEncodin…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ positional_encod… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m33,472\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m33,472\u001b[0m │ transformer_bloc… │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_block_2 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m33,472\u001b[0m │ transformer_bloc… │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │         \u001b[38;5;34m65\u001b[0m │ transformer_bloc… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ softmax (\u001b[38;5;33mSoftmax\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multiply (\u001b[38;5;33mMultiply\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ transformer_bloc… │\n",
       "│                     │                   │            │ softmax[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ multiply[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m8,320\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dropout_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │      \u001b[38;5;34m1,235\u001b[0m │ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">119,188</span> (465.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m119,188\u001b[0m (465.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">118,804</span> (464.08 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m118,804\u001b[0m (464.08 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752965145.875352      69 service.cc:148] XLA service 0x7fc9280043d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1752965145.875988      69 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1752965147.481250      69 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   7/8959\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:04\u001b[0m 21ms/step - accuracy: 0.0161 - loss: 3.1155 - top_k_categorical_accuracy: 0.2290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752965157.413758      69 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.2469 - loss: 2.3856 - top_k_categorical_accuracy: 0.6806\n",
      "Epoch 1: val_accuracy improved from -inf to 0.35878, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 22ms/step - accuracy: 0.2469 - loss: 2.3855 - top_k_categorical_accuracy: 0.6806 - val_accuracy: 0.3588 - val_loss: 2.0655 - val_top_k_categorical_accuracy: 0.8336 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4369 - loss: 1.7991 - top_k_categorical_accuracy: 0.8457\n",
      "Epoch 2: val_accuracy did not improve from 0.35878\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 20ms/step - accuracy: 0.4369 - loss: 1.7991 - top_k_categorical_accuracy: 0.8457 - val_accuracy: 0.3303 - val_loss: 2.4789 - val_top_k_categorical_accuracy: 0.7689 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5049 - loss: 1.6243 - top_k_categorical_accuracy: 0.8715\n",
      "Epoch 3: val_accuracy improved from 0.35878 to 0.39993, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 21ms/step - accuracy: 0.5049 - loss: 1.6243 - top_k_categorical_accuracy: 0.8715 - val_accuracy: 0.3999 - val_loss: 2.3130 - val_top_k_categorical_accuracy: 0.7923 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5337 - loss: 1.5320 - top_k_categorical_accuracy: 0.8857\n",
      "Epoch 4: val_accuracy improved from 0.39993 to 0.42015, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 21ms/step - accuracy: 0.5337 - loss: 1.5320 - top_k_categorical_accuracy: 0.8857 - val_accuracy: 0.4202 - val_loss: 2.3443 - val_top_k_categorical_accuracy: 0.8062 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5569 - loss: 1.4650 - top_k_categorical_accuracy: 0.8935\n",
      "Epoch 5: val_accuracy improved from 0.42015 to 0.53693, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 20ms/step - accuracy: 0.5569 - loss: 1.4650 - top_k_categorical_accuracy: 0.8935 - val_accuracy: 0.5369 - val_loss: 1.8047 - val_top_k_categorical_accuracy: 0.8533 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5738 - loss: 1.4121 - top_k_categorical_accuracy: 0.9034\n",
      "Epoch 6: val_accuracy did not improve from 0.53693\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 20ms/step - accuracy: 0.5738 - loss: 1.4121 - top_k_categorical_accuracy: 0.9034 - val_accuracy: 0.4882 - val_loss: 2.0223 - val_top_k_categorical_accuracy: 0.8410 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5851 - loss: 1.3631 - top_k_categorical_accuracy: 0.9092\n",
      "Epoch 7: val_accuracy did not improve from 0.53693\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 21ms/step - accuracy: 0.5851 - loss: 1.3631 - top_k_categorical_accuracy: 0.9092 - val_accuracy: 0.4960 - val_loss: 2.0771 - val_top_k_categorical_accuracy: 0.8218 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6008 - loss: 1.3287 - top_k_categorical_accuracy: 0.9120\n",
      "Epoch 8: val_accuracy did not improve from 0.53693\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 20ms/step - accuracy: 0.6008 - loss: 1.3287 - top_k_categorical_accuracy: 0.9120 - val_accuracy: 0.3204 - val_loss: 3.2648 - val_top_k_categorical_accuracy: 0.7228 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6086 - loss: 1.2988 - top_k_categorical_accuracy: 0.9150\n",
      "Epoch 9: val_accuracy did not improve from 0.53693\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 21ms/step - accuracy: 0.6086 - loss: 1.2988 - top_k_categorical_accuracy: 0.9150 - val_accuracy: 0.4587 - val_loss: 2.3240 - val_top_k_categorical_accuracy: 0.7833 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6167 - loss: 1.2692 - top_k_categorical_accuracy: 0.9193\n",
      "Epoch 10: val_accuracy did not improve from 0.53693\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 21ms/step - accuracy: 0.6167 - loss: 1.2692 - top_k_categorical_accuracy: 0.9193 - val_accuracy: 0.4129 - val_loss: 2.5061 - val_top_k_categorical_accuracy: 0.7863 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6245 - loss: 1.2535 - top_k_categorical_accuracy: 0.9228\n",
      "Epoch 11: val_accuracy improved from 0.53693 to 0.55997, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 21ms/step - accuracy: 0.6245 - loss: 1.2535 - top_k_categorical_accuracy: 0.9228 - val_accuracy: 0.5600 - val_loss: 1.6310 - val_top_k_categorical_accuracy: 0.8920 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6300 - loss: 1.2379 - top_k_categorical_accuracy: 0.9240\n",
      "Epoch 12: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 21ms/step - accuracy: 0.6300 - loss: 1.2379 - top_k_categorical_accuracy: 0.9240 - val_accuracy: 0.2802 - val_loss: 3.7608 - val_top_k_categorical_accuracy: 0.7550 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6317 - loss: 1.2224 - top_k_categorical_accuracy: 0.9246\n",
      "Epoch 13: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 21ms/step - accuracy: 0.6317 - loss: 1.2224 - top_k_categorical_accuracy: 0.9246 - val_accuracy: 0.3509 - val_loss: 2.9475 - val_top_k_categorical_accuracy: 0.7865 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6361 - loss: 1.2096 - top_k_categorical_accuracy: 0.9271\n",
      "Epoch 14: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 21ms/step - accuracy: 0.6361 - loss: 1.2096 - top_k_categorical_accuracy: 0.9271 - val_accuracy: 0.3600 - val_loss: 2.8781 - val_top_k_categorical_accuracy: 0.8092 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6440 - loss: 1.1921 - top_k_categorical_accuracy: 0.9281\n",
      "Epoch 15: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 20ms/step - accuracy: 0.6440 - loss: 1.1921 - top_k_categorical_accuracy: 0.9281 - val_accuracy: 0.3171 - val_loss: 3.5367 - val_top_k_categorical_accuracy: 0.7024 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6495 - loss: 1.1753 - top_k_categorical_accuracy: 0.9297\n",
      "Epoch 16: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 20ms/step - accuracy: 0.6495 - loss: 1.1753 - top_k_categorical_accuracy: 0.9297 - val_accuracy: 0.4981 - val_loss: 2.0284 - val_top_k_categorical_accuracy: 0.8567 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6503 - loss: 1.1681 - top_k_categorical_accuracy: 0.9312\n",
      "Epoch 17: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 21ms/step - accuracy: 0.6503 - loss: 1.1681 - top_k_categorical_accuracy: 0.9312 - val_accuracy: 0.2103 - val_loss: 4.4354 - val_top_k_categorical_accuracy: 0.6892 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6533 - loss: 1.1591 - top_k_categorical_accuracy: 0.9308\n",
      "Epoch 18: val_accuracy did not improve from 0.55997\n",
      "\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 20ms/step - accuracy: 0.6533 - loss: 1.1591 - top_k_categorical_accuracy: 0.9308 - val_accuracy: 0.3554 - val_loss: 3.4122 - val_top_k_categorical_accuracy: 0.6706 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6883 - loss: 1.0350 - top_k_categorical_accuracy: 0.9418\n",
      "Epoch 19: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 20ms/step - accuracy: 0.6883 - loss: 1.0350 - top_k_categorical_accuracy: 0.9418 - val_accuracy: 0.3767 - val_loss: 2.9064 - val_top_k_categorical_accuracy: 0.7949 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6978 - loss: 0.9974 - top_k_categorical_accuracy: 0.9460\n",
      "Epoch 20: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 21ms/step - accuracy: 0.6978 - loss: 0.9974 - top_k_categorical_accuracy: 0.9460 - val_accuracy: 0.5485 - val_loss: 2.0044 - val_top_k_categorical_accuracy: 0.8806 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7057 - loss: 0.9741 - top_k_categorical_accuracy: 0.9490\n",
      "Epoch 21: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 20ms/step - accuracy: 0.7057 - loss: 0.9741 - top_k_categorical_accuracy: 0.9490 - val_accuracy: 0.4675 - val_loss: 2.4798 - val_top_k_categorical_accuracy: 0.8400 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7061 - loss: 0.9676 - top_k_categorical_accuracy: 0.9511\n",
      "Epoch 22: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 21ms/step - accuracy: 0.7061 - loss: 0.9676 - top_k_categorical_accuracy: 0.9511 - val_accuracy: 0.4159 - val_loss: 2.8360 - val_top_k_categorical_accuracy: 0.8132 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7130 - loss: 0.9492 - top_k_categorical_accuracy: 0.9508\n",
      "Epoch 23: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 21ms/step - accuracy: 0.7130 - loss: 0.9492 - top_k_categorical_accuracy: 0.9508 - val_accuracy: 0.3189 - val_loss: 3.7878 - val_top_k_categorical_accuracy: 0.7397 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7106 - loss: 0.9486 - top_k_categorical_accuracy: 0.9525\n",
      "Epoch 24: val_accuracy did not improve from 0.55997\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 21ms/step - accuracy: 0.7106 - loss: 0.9486 - top_k_categorical_accuracy: 0.9525 - val_accuracy: 0.4325 - val_loss: 2.8104 - val_top_k_categorical_accuracy: 0.7999 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7125 - loss: 0.9434 - top_k_categorical_accuracy: 0.9517\n",
      "Epoch 25: val_accuracy did not improve from 0.55997\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 21ms/step - accuracy: 0.7125 - loss: 0.9434 - top_k_categorical_accuracy: 0.9517 - val_accuracy: 0.4698 - val_loss: 2.4290 - val_top_k_categorical_accuracy: 0.8310 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7350 - loss: 0.8720 - top_k_categorical_accuracy: 0.9578\n",
      "Epoch 26: val_accuracy improved from 0.55997 to 0.57052, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 21ms/step - accuracy: 0.7350 - loss: 0.8720 - top_k_categorical_accuracy: 0.9578 - val_accuracy: 0.5705 - val_loss: 1.9121 - val_top_k_categorical_accuracy: 0.8943 - learning_rate: 2.5000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7419 - loss: 0.8388 - top_k_categorical_accuracy: 0.9605\n",
      "Epoch 27: val_accuracy did not improve from 0.57052\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 21ms/step - accuracy: 0.7419 - loss: 0.8388 - top_k_categorical_accuracy: 0.9605 - val_accuracy: 0.5237 - val_loss: 2.2277 - val_top_k_categorical_accuracy: 0.8721 - learning_rate: 2.5000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7449 - loss: 0.8351 - top_k_categorical_accuracy: 0.9620\n",
      "Epoch 28: val_accuracy did not improve from 0.57052\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 21ms/step - accuracy: 0.7449 - loss: 0.8351 - top_k_categorical_accuracy: 0.9620 - val_accuracy: 0.5533 - val_loss: 2.1538 - val_top_k_categorical_accuracy: 0.8739 - learning_rate: 2.5000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7479 - loss: 0.8252 - top_k_categorical_accuracy: 0.9615\n",
      "Epoch 29: val_accuracy did not improve from 0.57052\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 21ms/step - accuracy: 0.7479 - loss: 0.8252 - top_k_categorical_accuracy: 0.9615 - val_accuracy: 0.4865 - val_loss: 2.5629 - val_top_k_categorical_accuracy: 0.8426 - learning_rate: 2.5000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7479 - loss: 0.8196 - top_k_categorical_accuracy: 0.9625\n",
      "Epoch 30: val_accuracy did not improve from 0.57052\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 21ms/step - accuracy: 0.7479 - loss: 0.8196 - top_k_categorical_accuracy: 0.9625 - val_accuracy: 0.4706 - val_loss: 2.7028 - val_top_k_categorical_accuracy: 0.8336 - learning_rate: 2.5000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7511 - loss: 0.8127 - top_k_categorical_accuracy: 0.9636\n",
      "Epoch 31: val_accuracy did not improve from 0.57052\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 21ms/step - accuracy: 0.7511 - loss: 0.8127 - top_k_categorical_accuracy: 0.9636 - val_accuracy: 0.4919 - val_loss: 2.4802 - val_top_k_categorical_accuracy: 0.8424 - learning_rate: 2.5000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7504 - loss: 0.8124 - top_k_categorical_accuracy: 0.9636\n",
      "Epoch 32: val_accuracy did not improve from 0.57052\n",
      "\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 21ms/step - accuracy: 0.7504 - loss: 0.8124 - top_k_categorical_accuracy: 0.9636 - val_accuracy: 0.5225 - val_loss: 2.4713 - val_top_k_categorical_accuracy: 0.8526 - learning_rate: 2.5000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7634 - loss: 0.7712 - top_k_categorical_accuracy: 0.9657\n",
      "Epoch 33: val_accuracy did not improve from 0.57052\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 21ms/step - accuracy: 0.7634 - loss: 0.7712 - top_k_categorical_accuracy: 0.9657 - val_accuracy: 0.5644 - val_loss: 2.1411 - val_top_k_categorical_accuracy: 0.8853 - learning_rate: 1.2500e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7678 - loss: 0.7580 - top_k_categorical_accuracy: 0.9669\n",
      "Epoch 34: val_accuracy did not improve from 0.57052\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 21ms/step - accuracy: 0.7678 - loss: 0.7580 - top_k_categorical_accuracy: 0.9669 - val_accuracy: 0.5621 - val_loss: 2.1525 - val_top_k_categorical_accuracy: 0.8869 - learning_rate: 1.2500e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7715 - loss: 0.7454 - top_k_categorical_accuracy: 0.9685\n",
      "Epoch 35: val_accuracy improved from 0.57052 to 0.58125, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 21ms/step - accuracy: 0.7715 - loss: 0.7454 - top_k_categorical_accuracy: 0.9685 - val_accuracy: 0.5813 - val_loss: 2.0169 - val_top_k_categorical_accuracy: 0.8962 - learning_rate: 1.2500e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7727 - loss: 0.7393 - top_k_categorical_accuracy: 0.9681\n",
      "Epoch 36: val_accuracy improved from 0.58125 to 0.59110, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 21ms/step - accuracy: 0.7727 - loss: 0.7393 - top_k_categorical_accuracy: 0.9681 - val_accuracy: 0.5911 - val_loss: 1.9419 - val_top_k_categorical_accuracy: 0.9022 - learning_rate: 1.2500e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7715 - loss: 0.7457 - top_k_categorical_accuracy: 0.9686\n",
      "Epoch 37: val_accuracy did not improve from 0.59110\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 21ms/step - accuracy: 0.7715 - loss: 0.7457 - top_k_categorical_accuracy: 0.9686 - val_accuracy: 0.5616 - val_loss: 2.1565 - val_top_k_categorical_accuracy: 0.8897 - learning_rate: 1.2500e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7727 - loss: 0.7355 - top_k_categorical_accuracy: 0.9696\n",
      "Epoch 38: val_accuracy did not improve from 0.59110\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 21ms/step - accuracy: 0.7727 - loss: 0.7355 - top_k_categorical_accuracy: 0.9696 - val_accuracy: 0.5892 - val_loss: 2.0525 - val_top_k_categorical_accuracy: 0.8938 - learning_rate: 1.2500e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7714 - loss: 0.7374 - top_k_categorical_accuracy: 0.9688\n",
      "Epoch 39: val_accuracy improved from 0.59110 to 0.60288, saving model to best_advanced_transformer.keras\n",
      "\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 21ms/step - accuracy: 0.7714 - loss: 0.7374 - top_k_categorical_accuracy: 0.9688 - val_accuracy: 0.6029 - val_loss: 1.9025 - val_top_k_categorical_accuracy: 0.9068 - learning_rate: 1.2500e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7786 - loss: 0.7137 - top_k_categorical_accuracy: 0.9696\n",
      "Epoch 40: val_accuracy did not improve from 0.60288\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 21ms/step - accuracy: 0.7786 - loss: 0.7137 - top_k_categorical_accuracy: 0.9696 - val_accuracy: 0.5895 - val_loss: 1.9652 - val_top_k_categorical_accuracy: 0.9098 - learning_rate: 6.2500e-05\n",
      "Epoch 41/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7805 - loss: 0.7068 - top_k_categorical_accuracy: 0.9714\n",
      "Epoch 41: val_accuracy did not improve from 0.60288\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 21ms/step - accuracy: 0.7805 - loss: 0.7068 - top_k_categorical_accuracy: 0.9714 - val_accuracy: 0.5969 - val_loss: 1.9452 - val_top_k_categorical_accuracy: 0.9054 - learning_rate: 6.2500e-05\n",
      "Epoch 42/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7838 - loss: 0.7014 - top_k_categorical_accuracy: 0.9715\n",
      "Epoch 42: val_accuracy improved from 0.60288 to 0.60552, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 21ms/step - accuracy: 0.7838 - loss: 0.7014 - top_k_categorical_accuracy: 0.9715 - val_accuracy: 0.6055 - val_loss: 1.9459 - val_top_k_categorical_accuracy: 0.9038 - learning_rate: 6.2500e-05\n",
      "Epoch 43/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7848 - loss: 0.6975 - top_k_categorical_accuracy: 0.9716\n",
      "Epoch 43: val_accuracy did not improve from 0.60552\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 21ms/step - accuracy: 0.7848 - loss: 0.6975 - top_k_categorical_accuracy: 0.9716 - val_accuracy: 0.5927 - val_loss: 1.9775 - val_top_k_categorical_accuracy: 0.9064 - learning_rate: 6.2500e-05\n",
      "Epoch 44/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7862 - loss: 0.6938 - top_k_categorical_accuracy: 0.9728\n",
      "Epoch 44: val_accuracy improved from 0.60552 to 0.61520, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 21ms/step - accuracy: 0.7862 - loss: 0.6938 - top_k_categorical_accuracy: 0.9728 - val_accuracy: 0.6152 - val_loss: 1.8477 - val_top_k_categorical_accuracy: 0.9140 - learning_rate: 6.2500e-05\n",
      "Epoch 45/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7856 - loss: 0.6937 - top_k_categorical_accuracy: 0.9722\n",
      "Epoch 45: val_accuracy did not improve from 0.61520\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 21ms/step - accuracy: 0.7856 - loss: 0.6937 - top_k_categorical_accuracy: 0.9722 - val_accuracy: 0.5990 - val_loss: 1.9485 - val_top_k_categorical_accuracy: 0.9101 - learning_rate: 6.2500e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7834 - loss: 0.6951 - top_k_categorical_accuracy: 0.9729\n",
      "Epoch 46: val_accuracy did not improve from 0.61520\n",
      "\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 21ms/step - accuracy: 0.7834 - loss: 0.6951 - top_k_categorical_accuracy: 0.9729 - val_accuracy: 0.6057 - val_loss: 1.8891 - val_top_k_categorical_accuracy: 0.9122 - learning_rate: 6.2500e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7890 - loss: 0.6813 - top_k_categorical_accuracy: 0.9729\n",
      "Epoch 47: val_accuracy did not improve from 0.61520\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 21ms/step - accuracy: 0.7890 - loss: 0.6813 - top_k_categorical_accuracy: 0.9729 - val_accuracy: 0.6140 - val_loss: 1.8370 - val_top_k_categorical_accuracy: 0.9149 - learning_rate: 3.1250e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7936 - loss: 0.6815 - top_k_categorical_accuracy: 0.9727\n",
      "Epoch 48: val_accuracy did not improve from 0.61520\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 21ms/step - accuracy: 0.7936 - loss: 0.6815 - top_k_categorical_accuracy: 0.9727 - val_accuracy: 0.6062 - val_loss: 1.9099 - val_top_k_categorical_accuracy: 0.9126 - learning_rate: 3.1250e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7925 - loss: 0.6736 - top_k_categorical_accuracy: 0.9725\n",
      "Epoch 49: val_accuracy did not improve from 0.61520\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 20ms/step - accuracy: 0.7925 - loss: 0.6736 - top_k_categorical_accuracy: 0.9725 - val_accuracy: 0.6080 - val_loss: 1.8873 - val_top_k_categorical_accuracy: 0.9108 - learning_rate: 3.1250e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7909 - loss: 0.6752 - top_k_categorical_accuracy: 0.9734\n",
      "Epoch 50: val_accuracy did not improve from 0.61520\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 20ms/step - accuracy: 0.7909 - loss: 0.6752 - top_k_categorical_accuracy: 0.9734 - val_accuracy: 0.6124 - val_loss: 1.8629 - val_top_k_categorical_accuracy: 0.9144 - learning_rate: 3.1250e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7912 - loss: 0.6717 - top_k_categorical_accuracy: 0.9735\n",
      "Epoch 51: val_accuracy did not improve from 0.61520\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 20ms/step - accuracy: 0.7912 - loss: 0.6717 - top_k_categorical_accuracy: 0.9735 - val_accuracy: 0.6133 - val_loss: 1.8995 - val_top_k_categorical_accuracy: 0.9128 - learning_rate: 3.1250e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7924 - loss: 0.6710 - top_k_categorical_accuracy: 0.9733\n",
      "Epoch 52: val_accuracy did not improve from 0.61520\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 20ms/step - accuracy: 0.7924 - loss: 0.6710 - top_k_categorical_accuracy: 0.9733 - val_accuracy: 0.6119 - val_loss: 1.9040 - val_top_k_categorical_accuracy: 0.9129 - learning_rate: 3.1250e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7917 - loss: 0.6717 - top_k_categorical_accuracy: 0.9728\n",
      "Epoch 53: val_accuracy did not improve from 0.61520\n",
      "\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7917 - loss: 0.6717 - top_k_categorical_accuracy: 0.9728 - val_accuracy: 0.6024 - val_loss: 1.9402 - val_top_k_categorical_accuracy: 0.9115 - learning_rate: 3.1250e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7936 - loss: 0.6652 - top_k_categorical_accuracy: 0.9734\n",
      "Epoch 54: val_accuracy improved from 0.61520 to 0.61959, saving model to best_advanced_transformer.keras\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7936 - loss: 0.6652 - top_k_categorical_accuracy: 0.9734 - val_accuracy: 0.6196 - val_loss: 1.8390 - val_top_k_categorical_accuracy: 0.9173 - learning_rate: 1.5625e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7935 - loss: 0.6631 - top_k_categorical_accuracy: 0.9744\n",
      "Epoch 55: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7935 - loss: 0.6631 - top_k_categorical_accuracy: 0.9744 - val_accuracy: 0.6133 - val_loss: 1.8700 - val_top_k_categorical_accuracy: 0.9151 - learning_rate: 1.5625e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7949 - loss: 0.6632 - top_k_categorical_accuracy: 0.9739\n",
      "Epoch 56: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7949 - loss: 0.6632 - top_k_categorical_accuracy: 0.9739 - val_accuracy: 0.6115 - val_loss: 1.8888 - val_top_k_categorical_accuracy: 0.9151 - learning_rate: 1.5625e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7957 - loss: 0.6563 - top_k_categorical_accuracy: 0.9754\n",
      "Epoch 57: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7957 - loss: 0.6563 - top_k_categorical_accuracy: 0.9754 - val_accuracy: 0.6140 - val_loss: 1.8646 - val_top_k_categorical_accuracy: 0.9175 - learning_rate: 1.5625e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7956 - loss: 0.6621 - top_k_categorical_accuracy: 0.9747\n",
      "Epoch 58: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7956 - loss: 0.6621 - top_k_categorical_accuracy: 0.9747 - val_accuracy: 0.6159 - val_loss: 1.8736 - val_top_k_categorical_accuracy: 0.9144 - learning_rate: 1.5625e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7957 - loss: 0.6593 - top_k_categorical_accuracy: 0.9743\n",
      "Epoch 59: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7957 - loss: 0.6593 - top_k_categorical_accuracy: 0.9743 - val_accuracy: 0.6134 - val_loss: 1.8742 - val_top_k_categorical_accuracy: 0.9140 - learning_rate: 1.5625e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7947 - loss: 0.6597 - top_k_categorical_accuracy: 0.9748\n",
      "Epoch 60: val_accuracy did not improve from 0.61959\n",
      "\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7947 - loss: 0.6597 - top_k_categorical_accuracy: 0.9748 - val_accuracy: 0.6124 - val_loss: 1.8731 - val_top_k_categorical_accuracy: 0.9145 - learning_rate: 1.5625e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7979 - loss: 0.6538 - top_k_categorical_accuracy: 0.9758\n",
      "Epoch 61: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7979 - loss: 0.6538 - top_k_categorical_accuracy: 0.9758 - val_accuracy: 0.6163 - val_loss: 1.8666 - val_top_k_categorical_accuracy: 0.9147 - learning_rate: 7.8125e-06\n",
      "Epoch 62/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7962 - loss: 0.6516 - top_k_categorical_accuracy: 0.9755\n",
      "Epoch 62: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7962 - loss: 0.6516 - top_k_categorical_accuracy: 0.9755 - val_accuracy: 0.6157 - val_loss: 1.8662 - val_top_k_categorical_accuracy: 0.9165 - learning_rate: 7.8125e-06\n",
      "Epoch 63/100\n",
      "\u001b[1m8957/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7977 - loss: 0.6541 - top_k_categorical_accuracy: 0.9752\n",
      "Epoch 63: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 19ms/step - accuracy: 0.7977 - loss: 0.6541 - top_k_categorical_accuracy: 0.9752 - val_accuracy: 0.6148 - val_loss: 1.8700 - val_top_k_categorical_accuracy: 0.9161 - learning_rate: 7.8125e-06\n",
      "Epoch 64/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7977 - loss: 0.6534 - top_k_categorical_accuracy: 0.9745\n",
      "Epoch 64: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7977 - loss: 0.6534 - top_k_categorical_accuracy: 0.9745 - val_accuracy: 0.6185 - val_loss: 1.8529 - val_top_k_categorical_accuracy: 0.9179 - learning_rate: 7.8125e-06\n",
      "Epoch 65/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7973 - loss: 0.6518 - top_k_categorical_accuracy: 0.9756\n",
      "Epoch 65: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 19ms/step - accuracy: 0.7973 - loss: 0.6518 - top_k_categorical_accuracy: 0.9756 - val_accuracy: 0.6154 - val_loss: 1.8686 - val_top_k_categorical_accuracy: 0.9158 - learning_rate: 7.8125e-06\n",
      "Epoch 66/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7966 - loss: 0.6542 - top_k_categorical_accuracy: 0.9751\n",
      "Epoch 66: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7966 - loss: 0.6542 - top_k_categorical_accuracy: 0.9751 - val_accuracy: 0.6148 - val_loss: 1.8867 - val_top_k_categorical_accuracy: 0.9159 - learning_rate: 7.8125e-06\n",
      "Epoch 67/100\n",
      "\u001b[1m8958/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7977 - loss: 0.6539 - top_k_categorical_accuracy: 0.9745\n",
      "Epoch 67: val_accuracy did not improve from 0.61959\n",
      "\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 20ms/step - accuracy: 0.7977 - loss: 0.6539 - top_k_categorical_accuracy: 0.9745 - val_accuracy: 0.6145 - val_loss: 1.8682 - val_top_k_categorical_accuracy: 0.9170 - learning_rate: 7.8125e-06\n",
      "Epoch 68/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7979 - loss: 0.6518 - top_k_categorical_accuracy: 0.9751\n",
      "Epoch 68: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 20ms/step - accuracy: 0.7979 - loss: 0.6518 - top_k_categorical_accuracy: 0.9751 - val_accuracy: 0.6166 - val_loss: 1.8578 - val_top_k_categorical_accuracy: 0.9168 - learning_rate: 3.9063e-06\n",
      "Epoch 69/100\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7976 - loss: 0.6487 - top_k_categorical_accuracy: 0.9758\n",
      "Epoch 69: val_accuracy did not improve from 0.61959\n",
      "\u001b[1m8959/8959\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 19ms/step - accuracy: 0.7976 - loss: 0.6487 - top_k_categorical_accuracy: 0.9758 - val_accuracy: 0.6161 - val_loss: 1.8625 - val_top_k_categorical_accuracy: 0.9173 - learning_rate: 3.9063e-06\n",
      "Epoch 69: early stopping\n",
      "Restoring model weights from the end of the best epoch: 54.\n",
      "\n",
      "Advanced Transformer Results:\n",
      "Test Accuracy: 0.6059\n",
      "Test Top-K Accuracy: 0.9200\n",
      "Precision: 0.6366\n",
      "Recall: 0.6059\n",
      "F1-Score: 0.6115\n",
      "Saved: advanced_transformer_confusion.png, advanced_transformer_confusion.npy, advanced_transformer_report.json\n",
      "\n",
      "Training completed successfully!\n",
      "Saved: advanced_transformer_model.keras, advanced_label_encoder.pkl, advanced_scaler.pkl\n",
      "\n",
      "2. Plotting Training History...\n",
      "\n",
      "3. Evaluating on Final Test Data...\n",
      "\n",
      "Found 19 GeoJSON files in /kaggle/input/final-test-data\n",
      "Processing file: needleleaf_douglas fir_douglas firmar-oct-2022.geojson, Rows: 506\n",
      "Processing file: broadleaf_short-lived deciduous_aldermar-oct-2022.geojson, Rows: 420\n",
      "Processing file: broadleaf_beech_european beechmar-oct-2022.geojson, Rows: 1703\n",
      "Processing file: needleleaf_larch_japanese larchmar-oct-2022.geojson, Rows: 135\n",
      "Processing file: broadleaf_short-lived deciduous_poplarmar-oct-2022.geojson, Rows: 77\n",
      "Processing file: needleleaf_pine_scots pinemar-oct-2022.geojson, Rows: 1202\n",
      "Processing file: broadleaf_oak_english oakmar-oct-2022.geojson, Rows: 645\n",
      "Processing file: needleleaf_larch_european larchmar-oct-2022.geojson, Rows: 221\n",
      "Processing file: broadleaf_long-lived deciduous_sycamore maplemar-oct-2022.geojson, Rows: 725\n",
      "Processing file: broadleaf_long-lived deciduous_european ashmar-oct-2022.geojson, Rows: 432\n",
      "Processing file: broadleaf_short-lived deciduous_birchmar-oct-2022.geojson, Rows: 353\n",
      "Processing file: broadleaf_long-lived deciduous_lindenmar-oct-2022.geojson, Rows: 51\n",
      "Processing file: broadleaf_oak_red oakmar-oct-2022.geojson, Rows: 381\n",
      "Processing file: needleleaf_spruce_norway sprucemar-oct-2022.geojson, Rows: 746\n",
      "Processing file: broadleaf_oak_sessile oakmar-oct-2022.geojson, Rows: 493\n",
      "Processing file: needleleaf_pine_weymouth pinemar-oct-2022.geojson, Rows: 22\n",
      "Processing file: needleleaf_fir_silver firmar-oct-2022.geojson, Rows: 173\n",
      "Processing file: broadleaf_long-lived deciduous_cherrymar-oct-2022.geojson, Rows: 57\n",
      "Processing file: needleleaf_pine_black pinemar-oct-2022.geojson, Rows: 9\n",
      "\n",
      "Total samples attempted: 8351\n",
      "Valid samples processed: 8351\n",
      "\n",
      "Bands with missing/None/parsing issues in test data:\n",
      "  B1_4: 6893 times\n",
      "  B2_4: 6893 times\n",
      "  B3_4: 6893 times\n",
      "  B4_4: 6893 times\n",
      "  B5_4: 6893 times\n",
      "  B6_4: 6893 times\n",
      "  B7_4: 6893 times\n",
      "  B8_4: 6893 times\n",
      "  B8A_4: 6893 times\n",
      "  B9_4: 6893 times\n",
      "  B11_4: 6893 times\n",
      "  B12_4: 6893 times\n",
      "  NDVI_4: 6893 times\n",
      "  EVI_4: 6893 times\n",
      "  SAVI_4: 6893 times\n",
      "  NDWI_4: 6893 times\n",
      "  DEM_4: 6759 times\n",
      "  B2_7: 4589 times\n",
      "  B3_7: 4589 times\n",
      "  B4_7: 4589 times\n",
      "  B5_7: 4589 times\n",
      "  B6_7: 4589 times\n",
      "  B7_7: 4589 times\n",
      "  B8_7: 4589 times\n",
      "  B8A_7: 4589 times\n",
      "  B11_7: 4589 times\n",
      "  B12_7: 4589 times\n",
      "  NDVI_7: 4589 times\n",
      "  EVI_7: 4589 times\n",
      "  SAVI_7: 4589 times\n",
      "  NDWI_7: 4589 times\n",
      "  B1_7: 4585 times\n",
      "  B9_7: 4585 times\n",
      "  DEM_7: 4147 times\n",
      "  B2_1: 3744 times\n",
      "  B3_1: 3744 times\n",
      "  B4_1: 3744 times\n",
      "  B5_1: 3744 times\n",
      "  B6_1: 3744 times\n",
      "  B7_1: 3744 times\n",
      "  B8_1: 3744 times\n",
      "  B8A_1: 3744 times\n",
      "  B11_1: 3744 times\n",
      "  B12_1: 3744 times\n",
      "  NDVI_1: 3744 times\n",
      "  EVI_1: 3744 times\n",
      "  SAVI_1: 3744 times\n",
      "  NDWI_1: 3744 times\n",
      "  B1_1: 3743 times\n",
      "  B9_1: 3743 times\n",
      "  DEM_1: 3423 times\n",
      "  B2_6: 645 times\n",
      "  B3_6: 645 times\n",
      "  B4_6: 645 times\n",
      "  B5_6: 645 times\n",
      "  B6_6: 645 times\n",
      "  B7_6: 645 times\n",
      "  B8_6: 645 times\n",
      "  B8A_6: 645 times\n",
      "  B11_6: 645 times\n",
      "  B12_6: 645 times\n",
      "  NDVI_6: 645 times\n",
      "  EVI_6: 645 times\n",
      "  SAVI_6: 645 times\n",
      "  NDWI_6: 645 times\n",
      "  B1_6: 644 times\n",
      "  B9_6: 644 times\n",
      "\n",
      "Evaluating model on final test data...\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step\n",
      "\n",
      "Final Test Data Accuracy: 0.5009\n",
      "Precision (Final Test): 0.5824\n",
      "Recall (Final Test): 0.5009\n",
      "F1-Score (Final Test): 0.5291\n",
      "\n",
      "Classification Report for Final Test Data:\n",
      "{\n",
      "    \"alder\": {\n",
      "        \"precision\": 0.4411764705882353,\n",
      "        \"recall\": 0.39285714285714285,\n",
      "        \"f1-score\": 0.4156171284634761,\n",
      "        \"support\": 420.0\n",
      "    },\n",
      "    \"birch\": {\n",
      "        \"precision\": 0.20388349514563106,\n",
      "        \"recall\": 0.17847025495750707,\n",
      "        \"f1-score\": 0.19033232628398794,\n",
      "        \"support\": 353.0\n",
      "    },\n",
      "    \"black pine\": {\n",
      "        \"precision\": 0.0,\n",
      "        \"recall\": 0.0,\n",
      "        \"f1-score\": 0.0,\n",
      "        \"support\": 9.0\n",
      "    },\n",
      "    \"cherry\": {\n",
      "        \"precision\": 0.012738853503184714,\n",
      "        \"recall\": 0.03508771929824561,\n",
      "        \"f1-score\": 0.01869158878504673,\n",
      "        \"support\": 57.0\n",
      "    },\n",
      "    \"douglas fir\": {\n",
      "        \"precision\": 0.696969696969697,\n",
      "        \"recall\": 0.6818181818181818,\n",
      "        \"f1-score\": 0.6893106893106892,\n",
      "        \"support\": 506.0\n",
      "    },\n",
      "    \"english oak\": {\n",
      "        \"precision\": 0.508955223880597,\n",
      "        \"recall\": 0.5286821705426357,\n",
      "        \"f1-score\": 0.5186311787072243,\n",
      "        \"support\": 645.0\n",
      "    },\n",
      "    \"european ash\": {\n",
      "        \"precision\": 0.2328519855595668,\n",
      "        \"recall\": 0.2986111111111111,\n",
      "        \"f1-score\": 0.26166328600405675,\n",
      "        \"support\": 432.0\n",
      "    },\n",
      "    \"european beech\": {\n",
      "        \"precision\": 0.8284023668639053,\n",
      "        \"recall\": 0.5754550792718731,\n",
      "        \"f1-score\": 0.6791406791406791,\n",
      "        \"support\": 1703.0\n",
      "    },\n",
      "    \"european larch\": {\n",
      "        \"precision\": 0.24349881796690306,\n",
      "        \"recall\": 0.4660633484162896,\n",
      "        \"f1-score\": 0.3198757763975155,\n",
      "        \"support\": 221.0\n",
      "    },\n",
      "    \"japanese larch\": {\n",
      "        \"precision\": 0.3333333333333333,\n",
      "        \"recall\": 0.6814814814814815,\n",
      "        \"f1-score\": 0.4476885644768856,\n",
      "        \"support\": 135.0\n",
      "    },\n",
      "    \"linden\": {\n",
      "        \"precision\": 0.08333333333333333,\n",
      "        \"recall\": 0.11764705882352941,\n",
      "        \"f1-score\": 0.09756097560975609,\n",
      "        \"support\": 51.0\n",
      "    },\n",
      "    \"norway spruce\": {\n",
      "        \"precision\": 0.761384335154827,\n",
      "        \"recall\": 0.5603217158176944,\n",
      "        \"f1-score\": 0.6455598455598456,\n",
      "        \"support\": 746.0\n",
      "    },\n",
      "    \"poplar\": {\n",
      "        \"precision\": 0.2556390977443609,\n",
      "        \"recall\": 0.44155844155844154,\n",
      "        \"f1-score\": 0.32380952380952377,\n",
      "        \"support\": 77.0\n",
      "    },\n",
      "    \"red oak\": {\n",
      "        \"precision\": 0.5324324324324324,\n",
      "        \"recall\": 0.5170603674540682,\n",
      "        \"f1-score\": 0.5246338215712383,\n",
      "        \"support\": 381.0\n",
      "    },\n",
      "    \"scots pine\": {\n",
      "        \"precision\": 0.7921225382932167,\n",
      "        \"recall\": 0.6023294509151415,\n",
      "        \"f1-score\": 0.6843100189035918,\n",
      "        \"support\": 1202.0\n",
      "    },\n",
      "    \"sessile oak\": {\n",
      "        \"precision\": 0.35628227194492257,\n",
      "        \"recall\": 0.4198782961460446,\n",
      "        \"f1-score\": 0.38547486033519557,\n",
      "        \"support\": 493.0\n",
      "    },\n",
      "    \"silver fir\": {\n",
      "        \"precision\": 0.43322475570032576,\n",
      "        \"recall\": 0.7687861271676301,\n",
      "        \"f1-score\": 0.5541666666666667,\n",
      "        \"support\": 173.0\n",
      "    },\n",
      "    \"sycamore maple\": {\n",
      "        \"precision\": 0.43646408839779005,\n",
      "        \"recall\": 0.32689655172413795,\n",
      "        \"f1-score\": 0.3738170347003155,\n",
      "        \"support\": 725.0\n",
      "    },\n",
      "    \"weymouth pine\": {\n",
      "        \"precision\": 0.0170316301703163,\n",
      "        \"recall\": 0.3181818181818182,\n",
      "        \"f1-score\": 0.03233256351039261,\n",
      "        \"support\": 22.0\n",
      "    },\n",
      "    \"accuracy\": 0.5008980960364028,\n",
      "    \"macro avg\": {\n",
      "        \"precision\": 0.37735393299908315,\n",
      "        \"recall\": 0.4163782272391039,\n",
      "        \"f1-score\": 0.3769798172755835,\n",
      "        \"support\": 8351.0\n",
      "    },\n",
      "    \"weighted avg\": {\n",
      "        \"precision\": 0.5823769122917755,\n",
      "        \"recall\": 0.5008980960364028,\n",
      "        \"f1-score\": 0.5290895924225794,\n",
      "        \"support\": 8351.0\n",
      "    }\n",
      "}\n",
      "\n",
      "Total Number of Test Points Evaluated: 8351\n",
      "Saved: advanced_transformer_final_test_report.json, advanced_transformer_final_test_confusion.npy, advanced_transformer_final_test_confusion.png\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETED!\n",
      "================================================================================\n",
      "\n",
      "Generated Files:\n",
      "- advanced_transformer_model.keras\n",
      "- advanced_label_encoder.pkl\n",
      "- advanced_scaler.pkl\n",
      "- advanced_transformer_report.json\n",
      "- advanced_transformer_confusion.png\n",
      "- advanced_transformer_confusion.npy\n",
      "- advanced_transformer_training_history.png\n",
      "- advanced_transformer_final_test_report.json\n",
      "- advanced_transformer_final_test_confusion.png\n",
      "- advanced_transformer_final_test_confusion.npy\n",
      "\n",
      "Final test evaluation: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import joblib\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import gc\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Enhanced Preprocessing Pipeline\n",
    "def preprocess_data(X, y, test_size=0.3, validation_size=0.5):\n",
    "    \"\"\"Enhanced preprocessing with better memory management.\"\"\"\n",
    "    # Encode class labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Flatten features for initial split\n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    # Initial train/temp split\n",
    "    X_train_raw, X_temp, y_train_raw, y_temp = train_test_split(\n",
    "        X_flat, y_encoded, test_size=test_size, stratify=y_encoded, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Apply SMOTE only to training set\n",
    "    print(\"Applying SMOTE to balance training data...\")\n",
    "    smote = SMOTE(random_state=42, k_neighbors=3)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_raw, y_train_raw)\n",
    "    \n",
    "    # Scale data\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_temp_scaled = scaler.transform(X_temp)\n",
    "    \n",
    "    # Split temp into val and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp_scaled, y_temp, test_size=validation_size, stratify=y_temp, random_state=42\n",
    "    )\n",
    "    \n",
    "    return (X_train_scaled, X_val, X_test, \n",
    "            y_train_resampled, y_val, y_test, \n",
    "            label_encoder, scaler)\n",
    "\n",
    "# 2. Advanced Transformer Architecture\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    \"\"\"Positional encoding for transformer.\"\"\"\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(max_len, embed_dim)\n",
    "    \n",
    "    def positional_encoding(self, max_len, embed_dim):\n",
    "        pos = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pos_encoding = np.zeros((max_len, embed_dim))\n",
    "        pos_encoding[:, 0::2] = np.sin(pos * div_term)\n",
    "        pos_encoding[:, 1::2] = np.cos(pos * div_term)\n",
    "        \n",
    "        return tf.constant(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:tf.shape(x)[1], :]\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    \"\"\"Enhanced multi-head self-attention with residual connections.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        \n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output\n",
    "    \n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        \n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        \n",
    "        attention = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"Enhanced transformer block with improved normalization and residual connections.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='gelu'),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def build_advanced_transformer(input_shape, num_classes, embed_dim=64, num_heads=8, \n",
    "                             ff_dim=128, num_blocks=3, dropout=0.15):\n",
    "    \"\"\"Build an advanced transformer model with multiple blocks.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    x = layers.Dense(embed_dim, kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
    "    \n",
    "    pos_encoding = PositionalEncoding(input_shape[0], embed_dim)\n",
    "    x = pos_encoding(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    for i in range(num_blocks):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout)(x)\n",
    "    \n",
    "    attention_weights = layers.Dense(1, activation='tanh')(x)\n",
    "    attention_weights = layers.Softmax(axis=1)(attention_weights)\n",
    "    x = layers.Multiply()([x, attention_weights])\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = layers.Dense(128, activation='gelu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='gelu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    \n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name='advanced_transformer')\n",
    "    return model\n",
    "\n",
    "# 3. Enhanced Training Pipeline\n",
    "def train_advanced_transformer(X_train, y_train, X_val, y_val, num_classes, \n",
    "                             sequence_length, class_weight_dict):\n",
    "    \"\"\"Train the advanced transformer with sophisticated callbacks.\"\"\"\n",
    "    \n",
    "    stride = max(1, X_train.shape[1] // sequence_length)\n",
    "    X_train_seq = X_train[:, ::stride][:, :sequence_length].reshape(-1, sequence_length, 1)\n",
    "    X_val_seq = X_val[:, ::stride][:, :sequence_length].reshape(-1, sequence_length, 1)\n",
    "    \n",
    "    y_train_onehot = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_val_onehot = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "    \n",
    "    model = build_advanced_transformer(\n",
    "        input_shape=(sequence_length, 1),\n",
    "        num_classes=num_classes,\n",
    "        embed_dim=64,\n",
    "        num_heads=8,\n",
    "        ff_dim=128,\n",
    "        num_blocks=3,\n",
    "        dropout=0.15\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999\n",
    "\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_advanced_transformer.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=7,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.TerminateOnNaN()\n",
    "    ]\n",
    "    \n",
    "    print(f\"Training model with {model.count_params():,} parameters...\")\n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_onehot,\n",
    "        validation_data=(X_val_seq, y_val_onehot),\n",
    "        epochs=100,\n",
    "        batch_size=8,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history, X_val_seq, y_val_onehot\n",
    "\n",
    "# 4. Enhanced Evaluation Function\n",
    "def evaluate_model(model, X_test, y_test, label_encoder, sequence_length):\n",
    "    \"\"\"Comprehensive model evaluation with multiple metrics.\"\"\"\n",
    "    \n",
    "    stride = max(1, X_test.shape[1] // sequence_length)\n",
    "    X_test_seq = X_test[:, ::stride][:, :sequence_length].reshape(-1, sequence_length, 1)\n",
    "    y_test_onehot = tf.keras.utils.to_categorical(y_test, len(label_encoder.classes_))\n",
    "    \n",
    "    test_loss, test_accuracy, test_top_k = model.evaluate(X_test_seq, y_test_onehot, verbose=0)\n",
    "    \n",
    "    y_pred_probs = model.predict(X_test_seq, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_, \n",
    "                                 output_dict=True, zero_division=0)\n",
    "    \n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nAdvanced Transformer Results:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Top-K Accuracy: {test_top_k:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Generate and save confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_, \n",
    "                yticklabels=label_encoder.classes_, cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Advanced Transformer - Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('advanced_transformer_confusion.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save confusion matrix as NumPy array\n",
    "    np.save('advanced_transformer_confusion.npy', cm)\n",
    "    \n",
    "    # Save classification report\n",
    "    with open('advanced_transformer_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    print(\"Saved: advanced_transformer_confusion.png, advanced_transformer_confusion.npy, advanced_transformer_report.json\")\n",
    "    \n",
    "    return report, y_pred_probs, y_pred\n",
    "\n",
    "# 5. Main Training Pipeline\n",
    "def main_training_pipeline(X, y):\n",
    "    \"\"\"Complete training pipeline with the advanced transformer.\"\"\"\n",
    "    \n",
    "    print(\"Starting advanced transformer training pipeline...\")\n",
    "    \n",
    "    (X_train_scaled, X_val, X_test, \n",
    "     y_train_resampled, y_val, y_test, \n",
    "     label_encoder, scaler) = preprocess_data(X, y)\n",
    "    \n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    sequence_length = min(512, X_train_scaled.shape[1] // 4)\n",
    "    \n",
    "    print(f\"Data shapes - Train: {X_train_scaled.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"Sequence length: {sequence_length}\")\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', \n",
    "                                       classes=np.unique(y_train_resampled), \n",
    "                                       y=y_train_resampled)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    model, history, X_val_seq, y_val_onehot = train_advanced_transformer(\n",
    "        X_train_scaled, y_train_resampled, X_val, y_val, \n",
    "        num_classes, sequence_length, class_weight_dict\n",
    "    )\n",
    "    \n",
    "    report, y_pred_probs, y_pred = evaluate_model(\n",
    "        model, X_test, y_test, label_encoder, sequence_length\n",
    "    )\n",
    "    \n",
    "    model.save('advanced_transformer_model.keras')\n",
    "    joblib.dump(label_encoder, 'advanced_label_encoder.pkl')\n",
    "    joblib.dump(scaler, 'advanced_scaler.pkl')\n",
    "    \n",
    "    with open('advanced_transformer_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    print(\"\\nTraining completed successfully!\")\n",
    "    print(\"Saved: advanced_transformer_model.keras, advanced_label_encoder.pkl, advanced_scaler.pkl\")\n",
    "    \n",
    "    return model, history, label_encoder, scaler\n",
    "\n",
    "# 6. Plot Training History\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    if 'learning_rate' in history.history:\n",
    "        ax3.plot(history.history['learning_rate'])\n",
    "        ax3.set_title('Learning Rate')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Learning Rate')\n",
    "        ax3.set_yscale('log')\n",
    "        ax3.grid(True)\n",
    "    \n",
    "    if 'top_k_categorical_accuracy' in history.history:\n",
    "        ax4.plot(history.history['top_k_categorical_accuracy'], label='Training Top-K')\n",
    "        ax4.plot(history.history['val_top_k_categorical_accuracy'], label='Validation Top-K')\n",
    "        ax4.set_title('Top-K Accuracy')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Top-K Accuracy')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('advanced_transformer_training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# 7. Updated Enhanced Test Data Evaluation\n",
    "def evaluate_on_final_test_data(model, label_encoder, scaler, sequence_length, test_data_dir=\"/kaggle/input/final-test-data\"):\n",
    "    \"\"\"Evaluate the trained transformer model on final test data with proper preprocessing.\"\"\"\n",
    "    \n",
    "    # Define band columns to match training data\n",
    "    bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'NDVI', 'EVI', 'SAVI', 'NDWI', 'DEM']\n",
    "    months = ['', '_1', '_2', '_3', '_4', '_5', '_6', '_7']\n",
    "    band_columns = [band + month for month in months for band in bands]\n",
    "    \n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "    test_invalid_samples = []\n",
    "    test_invalid_bands = Counter()\n",
    "    total_samples_attempted = 0\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(test_data_dir):\n",
    "        print(f\"\\nError: Test data directory {test_data_dir} does not exist.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    geojson_files = [f for f in os.listdir(test_data_dir) if f.endswith(\".geojson\")]\n",
    "    if not geojson_files:\n",
    "        print(f\"\\nError: No GeoJSON files found in {test_data_dir}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"\\nFound {len(geojson_files)} GeoJSON files in {test_data_dir}\")\n",
    "    \n",
    "    # Load all GeoJSON files\n",
    "    for file in geojson_files:\n",
    "        try:\n",
    "            file_path = os.path.join(test_data_dir, file)\n",
    "            gdf = gpd.read_file(file_path)\n",
    "            print(f\"Processing file: {file}, Rows: {len(gdf)}\")\n",
    "            total_samples_attempted += len(gdf)\n",
    "            \n",
    "            for idx, row in gdf.iterrows():\n",
    "                try:\n",
    "                    patch = []\n",
    "                    for col in band_columns:\n",
    "                        if col not in gdf.columns:\n",
    "                            test_invalid_bands[col] += 1\n",
    "                            array = np.zeros((5, 5), dtype=np.float32)\n",
    "                            patch.append(array)\n",
    "                            continue\n",
    "                        \n",
    "                        data = row[col]\n",
    "                        if data is None or (isinstance(data, str) and data.lower() == 'none'):\n",
    "                            test_invalid_bands[col] += 1\n",
    "                            array = np.zeros((5, 5), dtype=np.float32)\n",
    "                            patch.append(array)\n",
    "                            continue\n",
    "                        \n",
    "                        try:\n",
    "                            if isinstance(data, str):\n",
    "                                parsed_data = ast.literal_eval(data)\n",
    "                            else:\n",
    "                                parsed_data = data\n",
    "                            array = np.array(parsed_data, dtype=np.float32).reshape(5, 5)\n",
    "                            if array.shape != (5, 5):\n",
    "                                raise ValueError(f\"Unexpected array shape for {col}: {array.shape}\")\n",
    "                        except (ValueError, SyntaxError, TypeError) as e:\n",
    "                            test_invalid_bands[col] += 1\n",
    "                            array = np.zeros((5, 5), dtype=np.float32)\n",
    "                            patch.append(array)\n",
    "                        \n",
    "                        patch.append(array)\n",
    "                    \n",
    "                    patch = np.stack(patch, axis=-1)\n",
    "                    if patch.shape != (5, 5, 136):\n",
    "                        raise ValueError(f\"Unexpected patch shape: {patch.shape}\")\n",
    "                    \n",
    "                    test_features.append(patch)\n",
    "                    test_labels.append(row['l3_species'])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    test_invalid_samples.append((file, idx, str(e)))\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process file {file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nTotal samples attempted: {total_samples_attempted}\")\n",
    "    print(f\"Valid samples processed: {len(test_features)}\")\n",
    "    \n",
    "    if not test_features:\n",
    "        print(\"\\nError: No valid test samples loaded. Cannot evaluate model.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if test_invalid_samples:\n",
    "        print(f\"\\nSkipped {len(test_invalid_samples)} invalid test samples:\")\n",
    "        for file, idx, error in test_invalid_samples:\n",
    "            print(f\"File: {file}, Row: {idx}, Error: {error}\")\n",
    "    \n",
    "    if test_invalid_bands:\n",
    "        print(\"\\nBands with missing/None/parsing issues in test data:\")\n",
    "        for band, count in test_invalid_bands.most_common():\n",
    "            print(f\"  {band}: {count} times\")\n",
    "    \n",
    "    # Convert to NumPy arrays\n",
    "    X_test_final = np.array(test_features, dtype=np.float32)  # Shape: (N, 5, 5, 136)\n",
    "    y_test_final = np.array(test_labels)\n",
    "    \n",
    "    # Handle unknown labels\n",
    "    try:\n",
    "        known_classes = set(label_encoder.classes_)\n",
    "        test_classes = set(y_test_final)\n",
    "        unknown_labels = test_classes - known_classes\n",
    "        \n",
    "        if unknown_labels:\n",
    "            print(f\"\\nWarning: Unknown labels in test data: {unknown_labels}\")\n",
    "            valid_mask = np.isin(y_test_final, list(known_classes))\n",
    "            X_test_final = X_test_final[valid_mask]\n",
    "            y_test_final = y_test_final[valid_mask]\n",
    "            print(f\"Filtered dataset size: {len(y_test_final)} samples\")\n",
    "        \n",
    "        if len(y_test_final) == 0:\n",
    "            print(\"Error: No valid labels found in test data after filtering.\")\n",
    "            return None, None, None\n",
    "        \n",
    "        y_test_final_encoded = label_encoder.transform(y_test_final)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in label encoding: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Preprocess test data\n",
    "    try:\n",
    "        # Flatten for scaling\n",
    "        X_test_final_flat = X_test_final.reshape(X_test_final.shape[0], -1)  # Shape: (N, 5*5*136)\n",
    "        X_test_final_scaled = scaler.transform(X_test_final_flat)  # Apply same scaler as training\n",
    "        \n",
    "        # Reshape for transformer input\n",
    "        stride = max(1, X_test_final_scaled.shape[1] // sequence_length)\n",
    "        X_test_final_seq = X_test_final_scaled[:, ::stride][:, :sequence_length].reshape(-1, sequence_length, 1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in data preprocessing: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Evaluate model\n",
    "    try:\n",
    "        print(\"\\nEvaluating model on final test data...\")\n",
    "        y_pred_final_probs = model.predict(X_test_final_seq, verbose=1)\n",
    "        y_pred_final_classes = np.argmax(y_pred_final_probs, axis=1)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        test_accuracy_final = (y_pred_final_classes == y_test_final_encoded).mean()\n",
    "        print(f\"\\nFinal Test Data Accuracy: {test_accuracy_final:.4f}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision_final = precision_score(y_test_final_encoded, y_pred_final_classes, \n",
    "                                        average='weighted', zero_division=0)\n",
    "        recall_final = recall_score(y_test_final_encoded, y_pred_final_classes, \n",
    "                                  average='weighted', zero_division=0)\n",
    "        f1_final = f1_score(y_test_final_encoded, y_pred_final_classes, \n",
    "                           average='weighted', zero_division=0)\n",
    "        \n",
    "        print(f\"Precision (Final Test): {precision_final:.4f}\")\n",
    "        print(f\"Recall (Final Test): {recall_final:.4f}\")\n",
    "        print(f\"F1-Score (Final Test): {f1_final:.4f}\")\n",
    "        \n",
    "        # Get unique labels in test data\n",
    "        unique_test_labels = np.unique(y_test_final_encoded)\n",
    "        unique_test_label_names = label_encoder.inverse_transform(unique_test_labels)\n",
    "        \n",
    "        # Classification report\n",
    "        report_final = classification_report(\n",
    "            y_test_final_encoded,\n",
    "            y_pred_final_classes,\n",
    "            labels=unique_test_labels,\n",
    "            target_names=unique_test_label_names,\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        print(\"\\nClassification Report for Final Test Data:\")\n",
    "        print(json.dumps(report_final, indent=4))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm_final = confusion_matrix(y_test_final_encoded, y_pred_final_classes, \n",
    "                                   labels=unique_test_labels)\n",
    "        \n",
    "        plt.figure(figsize=(max(10, len(unique_test_labels)), max(8, len(unique_test_labels))))\n",
    "        sns.heatmap(cm_final, annot=True, fmt='d', \n",
    "                   xticklabels=unique_test_label_names, \n",
    "                   yticklabels=unique_test_label_names, \n",
    "                   cmap='Blues', square=True)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Advanced Transformer - Final Test Data Confusion Matrix')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('advanced_transformer_final_test_confusion.png', \n",
    "                   dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Save outputs\n",
    "        with open('advanced_transformer_final_test_report.json', 'w') as f:\n",
    "            json.dump(report_final, f, indent=4)\n",
    "        np.save('advanced_transformer_final_test_confusion.npy', cm_final)\n",
    "        \n",
    "        print(f\"\\nTotal Number of Test Points Evaluated: {len(y_test_final)}\")\n",
    "        print(\"Saved: advanced_transformer_final_test_report.json, advanced_transformer_final_test_confusion.npy, advanced_transformer_final_test_confusion.png\")\n",
    "        \n",
    "        return report_final, y_pred_final_probs, y_pred_final_classes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# 8. Complete Training and Evaluation Pipeline\n",
    "def complete_pipeline(X, y, test_data_dir=\"/kaggle/input/final-test-data\"):\n",
    "    \"\"\"Complete pipeline with better error handling.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ADVANCED TRANSFORMER TRAINING AND EVALUATION PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Train the model\n",
    "        print(\"\\n1. Training Advanced Transformer Model...\")\n",
    "        model, history, label_encoder, scaler = main_training_pipeline(X, y)\n",
    "        \n",
    "        # Step 2: Plot training history\n",
    "        print(\"\\n2. Plotting Training History...\")\n",
    "        plot_training_history(history)\n",
    "        \n",
    "        # Step 3: Evaluate on final test data\n",
    "        print(\"\\n3. Evaluating on Final Test Data...\")\n",
    "        sequence_length = min(512, X.reshape(X.shape[0], -1).shape[1] // 4)\n",
    "        \n",
    "        final_report, final_probs, final_preds = evaluate_on_final_test_data(\n",
    "            model, label_encoder, scaler, sequence_length, test_data_dir\n",
    "        )\n",
    "        \n",
    "        # Step 4: Summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"PIPELINE COMPLETED!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nGenerated Files:\")\n",
    "        print(\"- advanced_transformer_model.keras\")\n",
    "        print(\"- advanced_label_encoder.pkl\")\n",
    "        print(\"- advanced_scaler.pkl\")\n",
    "        print(\"- advanced_transformer_report.json\")\n",
    "        print(\"- advanced_transformer_confusion.png\")\n",
    "        print(\"- advanced_transformer_confusion.npy\")\n",
    "        print(\"- advanced_transformer_training_history.png\")\n",
    "        \n",
    "        if final_report is not None:\n",
    "            print(\"- advanced_transformer_final_test_report.json\")\n",
    "            print(\"- advanced_transformer_final_test_confusion.png\")\n",
    "            print(\"- advanced_transformer_final_test_confusion.npy\")\n",
    "            print(\"\\nFinal test evaluation: SUCCESS\")\n",
    "        else:\n",
    "            print(\"\\nFinal test evaluation: SKIPPED (no test data or errors occurred)\")\n",
    "        \n",
    "        return model, history, label_encoder, scaler, final_report\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None, None\n",
    "\n",
    "model, history, label_encoder, scaler, final_report = complete_pipeline(X, y)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7762517,
     "sourceId": 12315204,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7871256,
     "sourceId": 12475667,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13249.198082,
   "end_time": "2025-07-20T02:19:41.821029",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-19T22:38:52.622947",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

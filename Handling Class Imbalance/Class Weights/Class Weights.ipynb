{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "2Zyp_drobDh9",
        "outputId": "a8703997-badf-4968-d16d-8200f9f268aa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/TreeSatAI1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1197346105>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Inspect first file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mfirst_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfirst_file\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfirst_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".geojson\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mgdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/TreeSatAI1'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import geopandas as gpd\n",
        "import ast\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Inspect and Load GeoJSON Files\n",
        "data_dir = \"/content/drive/MyDrive/TreeSatAI1\"  # Replace with your folder path\n",
        "all_features = []\n",
        "all_labels = []\n",
        "invalid_samples = []\n",
        "invalid_bands = Counter()\n",
        "species_counts = Counter()\n",
        "\n",
        "bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12', 'NDVI', 'EVI', 'EVI2', 'SAVI', 'NDWI']\n",
        "months = ['', '_1', '_2', '_3', '_4', '_5', '_6', '_7']\n",
        "band_columns = [band + month for month in months for band in bands]\n",
        "\n",
        "# Inspect first file\n",
        "first_file = os.path.join(data_dir, os.listdir(data_dir)[0]) if os.listdir(data_dir) else None\n",
        "if first_file and first_file.endswith(\".geojson\"):\n",
        "    gdf = gpd.read_file(first_file)\n",
        "    print(\"Inspecting first 2 rows of first GeoJSON file:\")\n",
        "    for idx in range(min(2, len(gdf))):\n",
        "        print(f\"\\nRow {idx}:\")\n",
        "        for band in ['B2', 'B11', 'NDVI', 'B2_1', 'NDVI_7']:\n",
        "            data = gdf[band].iloc[idx]\n",
        "            try:\n",
        "                parsed_data = ast.literal_eval(data) if isinstance(data, str) else data\n",
        "                array = np.array(parsed_data, dtype=np.float32)\n",
        "                print(f\"  Band {band}: shape={array.shape}, first few values={array.flatten()[:5]}\")\n",
        "            except (ValueError, SyntaxError, TypeError) as e:\n",
        "                print(f\"  Band {band}: Error parsing/converting: {e}\")\n",
        "\n",
        "# Load all GeoJSON files\n",
        "for file in os.listdir(data_dir):\n",
        "    if file.endswith(\".geojson\"):\n",
        "        gdf = gpd.read_file(os.path.join(data_dir, file))\n",
        "        for idx, row in gdf.iterrows():\n",
        "            try:\n",
        "                patch = []\n",
        "                for col in band_columns:\n",
        "                    data = row[col]\n",
        "                    if data is None or (isinstance(data, str) and data.lower() == 'none'):\n",
        "                        invalid_bands[col] += 1\n",
        "                        raise ValueError(f\"None value in band {col}\")\n",
        "                    parsed_data = ast.literal_eval(data) if isinstance(data, str) else data\n",
        "                    array = np.array(parsed_data, dtype=np.float32).reshape(5, 5)\n",
        "                    patch.append(array)\n",
        "                patch = np.stack(patch, axis=-1)\n",
        "                if patch.shape != (5, 5, 120):\n",
        "                    raise ValueError(f\"Unexpected patch shape: {patch.shape}\")\n",
        "                all_features.append(patch)\n",
        "                all_labels.append(row['l3_species'])\n",
        "                species_counts[row['l3_species']] += 1\n",
        "            except (ValueError, SyntaxError, TypeError) as e:\n",
        "                invalid_samples.append((file, idx, str(e)))\n",
        "                continue\n",
        "\n",
        "# Log invalid samples and bands\n",
        "if invalid_samples:\n",
        "    print(f\"\\nSkipped {len(invalid_samples)} invalid samples:\")\n",
        "    for file, idx, error in invalid_samples[:5]:\n",
        "        print(f\"File: {file}, Row: {idx}, Error: {error}\")\n",
        "if invalid_bands:\n",
        "    print(\"\\nBands with None values:\")\n",
        "    for band, count in invalid_bands.most_common():\n",
        "        print(f\"  {band}: {count} times\")\n",
        "print(\"\\nValid samples per species:\")\n",
        "for species, count in species_counts.most_common():\n",
        "    print(f\"  {species}: {count}\")\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "if not all_features:\n",
        "    print(\"\\nError: No valid samples loaded. Using Random Forest with dummy data.\")\n",
        "    X_dummy = np.random.rand(100, 5*5*120)\n",
        "    y_dummy = np.random.randint(0, 5, 100)\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_dummy, y_dummy)\n",
        "    print(\"Random Forest dummy accuracy:\", rf.score(X_dummy, y_dummy))\n",
        "    print(\"Please re-export data with updated GEE code.\")\n",
        "    exit()\n",
        "\n",
        "X = np.array(all_features, dtype=np.float32)  # Shape: (21868, 5, 5, 120)\n",
        "y = np.array(all_labels)\n",
        "\n",
        "# 2. Preprocess Data\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_onehot = tf.keras.utils.to_categorical(y_encoded)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "print(f\"\\nData shape: {X.shape}, Number of classes: {num_classes}\")\n",
        "print(f\"Data range: min={X.min():.4f}, max={X.max():.4f}\")\n",
        "\n",
        "# Compute class weights for imbalance\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_encoded), y=y_encoded)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Train-test-validation split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.15, random_state=42, stratify=y_onehot)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1765, random_state=42, stratify=y_train)\n",
        "\n",
        "# 3. Define CNN Model\n",
        "def build_cnn(input_shape=(5, 5, 120), num_classes=num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_cnn()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. Data Augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "# 5. Train Model\n",
        "history = model.fit(\n",
        "    data_augmentation(X_train), y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=64,  # Increased for larger dataset\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 6. Evaluate Model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# 7. Additional Metrics\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_classes, y_pred_classes, target_names=label_encoder.classes_))\n",
        "precision = precision_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "recall = recall_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')\n",
        "print(f\"Weighted Precision: {precision:.4f}\")\n",
        "print(f\"Weighted Recall: {recall:.4f}\")\n",
        "print(f\"Weighted F1-Score: {f1:.4f}\")\n",
        "\n",
        "# 8. Confusion Matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (CNN)')\n",
        "plt.show()\n",
        "\n",
        "# 9. Save Label Encoder\n",
        "import joblib\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}